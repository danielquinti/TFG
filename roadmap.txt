multiworker strategy
En las apiladas baja la pérdida en validación, en el resto no

Para hacer un modelo generativo necesito que este funcione bien antes o se puede hacer en paralelo?
La pérdida aumenta en validación

600k ejemplos con ws=10 vs 176k
Favorecer secuencias más largas?

subir capacidad de stacked
regularizacion al simple en la capa de salida
librería optuna optimizar hiperparámetros 5% datos separar en test/train

https://keras.io/examples/timeseries/timeseries_transformer_classification/
https://paperswithcode.com/paper/attention-is-all-you-need
entrenar embeddings por separado
Para qué sirve la capa densa de la salida?


Qué hace el parser viejo?
Intenta leer las canciones
Busca una pista cuyo nombre contuviera la palabra "guitar" pero sin acordes.
Codifica la música en 12 notas, 1 silencio y 7 valores de duración.
Si hay demasiados silencios, corta la pista en dos trozos.
Si un trozo es demasiado corto, lo descarto


Qué queremos que haga el nuevo?
Buscar pistas mélódicas, mientras no sea percusión me vale
Extraer varias pistas de cada canción
Extraer la nota y la octava de note.realvalue
Extraer la duración del beat OJO comprobar si está dotted





0. Clean tabs: remove broken or unsupported files from 60Ktabs
1.song_processor
	50k gp files -> 90K npy files (chunks)
		[note[0,12],octave[0,10],dur_log[0,7],is_dotted(0..1)]
		[note,octave,dur_log,is_dotted]
		[note,octave,dur_log,is_dotted]
		...
	split into big enough sequences (60-1000) without lots of rests
	Ispercussiontrack quitar
	group by midi instrument, song, track, chunk
2.jupyter
	get insight
		instrument distribution
		chunk length distribution
		Distribución de notas, octavas y duraciones
	select interesting instruments
	npy files -> shuffle -> split into train and test folders
	for each model configuration
		apply windowing


3.model_trainer(all in the same python execution)
	for each model configuration
		select the label features we want to train (notes/dur_log/both)
		load windowed dataset and weights
		remove empty classes?
		for training inputs
			apply one-hot encoding or embedding and concatenate beat features into a single vector representation
				input[....], label[.]
			feed into the model
		optuna tuning


semitones
	0(rest), 1,2,3,4,5,6,7,8,9,10,11,12

octave
	0(rest), 1,2,3,4,5,6,7,8,9,10
dur_log=0,1,2,3,4,5,6
dotted=bool



Pre-model options
    all instruments
	one specific instrument
	narrow chunk length window

Input
	1.            2.             3.              4.      
	semitone      semitone       pitch           pitch        
	octave        dur_log        duration        dotted          
	dur_log
	dotted

Preprocessing (Embedding)
1.                      2.                       3.
OHE->concat->Dense      Single-value->Embedding  Pretrain embeddings

Output
""              ""                ""            ""
                        



Model
1.                    2.   3.   4.      5.       
Last note in window   FFWD LSTM STACKED ATTENTION 


Loss function
1.                             2.                 3.                            4.
CCE(multiclass classification) WCCE(weighted CCE) L-MSE(logarithmic regression) BCE

Metric
1.        2.
Accuracy  Balanced accuracy



Dudas:
flatten antes de dense?
Cómo navegamos el espacio de configuraciones?
Enseñar preprocessing de labels e inputs
sparse categorical accuracy?
Multihot?
pesos on the fly, ba, wcce
pérdida?


codif 1 prep cuatro embeddings por separado concatenar

tf dataset weight mapping
class weights
empezar con cce
dotted->precision e recall

log inicial, intermedio y final

graphviz pydot optuna