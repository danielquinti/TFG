{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6681113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fcd422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_paths(route):\n",
    "    name_list = []\n",
    "    for root, dirs, files in os.walk(route):\n",
    "        for file in files:\n",
    "            name_list.append(os.path.join(root, file))\n",
    "    if not name_list:\n",
    "        raise FileNotFoundError(\"\")\n",
    "    return name_list\n",
    "\n",
    "\n",
    "def merge_data(path):\n",
    "    file_paths = get_file_paths(path)\n",
    "    return [np.load(file_path) for file_path in file_paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c815433",
   "metadata": {},
   "source": [
    "0. Clean tabs: remove broken or unsupported files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048a8647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "def dump_files(files_dict):\n",
    "    os.makedirs(os.path.join(\"clean_tabs\", \"gp3\"))\n",
    "    os.makedirs(os.path.join(\"clean_tabs\", \"gp4\"))\n",
    "    os.makedirs(os.path.join(\"clean_tabs\", \"gp5\"))\n",
    "    os.makedirs(os.path.join(\"clean_tabs\", \"gtp\"))\n",
    "\n",
    "    for idx, full_path in enumerate(files_dict.values()):\n",
    "        shutil.copy(\n",
    "            full_path,\n",
    "            os.path.join(\n",
    "                \"clean_tabs\",\n",
    "                os.path.splitext(os.path.basename(full_path))[1][1:],\n",
    "                os.path.basename(full_path)\n",
    "            )\n",
    "        )\n",
    "\n",
    "def get_unique_file_dict(path_list):\n",
    "    def is_acceptable(song_path, fmt):\n",
    "        if fmt not in ('.gp3', '.gp4', '.gp5', \".gtp\") \\\n",
    "                or os.path.getsize(song_path) <= 1024:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    files_dict = {}\n",
    "    for filepath in path_list:\n",
    "        # avoid duplicates\n",
    "        name = os.path.basename(filepath)\n",
    "        k, extension = os.path.splitext(name)\n",
    "        if is_acceptable(filepath, extension) and not files_dict.get(k):\n",
    "            files_dict[k] = filepath\n",
    "    return files_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fe9568",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = get_file_paths(\"dirty_tabs\")\n",
    "print(\"Files in folder: {}\".format(len(filepaths)))\n",
    "files_dict = get_unique_file_dict(filepaths)\n",
    "print(\"Unique files in folder: {}\".format(len(files_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa41c647",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_files(files_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e08c73",
   "metadata": {},
   "source": [
    "1. Split into chunks\n",
    "2. Get chunk insight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2c802c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_chunk_dict={instrument: merge_data(os.path.join(\"inst_grouped\",instrument)) for instrument in os.listdir(\"inst_grouped\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd28636",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = np.array([chunk.shape[0] for chunk in data])\n",
    "import plotly.express as px\n",
    "fig = px.histogram(lengths)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b26db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(lengths))\n",
    "print(np.sum(lengths[lengths >1000]))\n",
    "print(np.sum(lengths[lengths <60]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdba351",
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk=np.concatenate(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e47d096",
   "metadata": {},
   "outputs": [],
   "source": [
    "note =bulk[:,0]\n",
    "octave=bulk[:,1]\n",
    "duration=bulk[:,2]\n",
    "dotted=bulk[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b9c0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(octave, return_counts=True)\n",
    "\n",
    "print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799ee341",
   "metadata": {},
   "source": [
    "2. Shuffle and split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e12ae65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "song_paths = [x for x in os.listdir(\"song_chunks/song_grouped/\")]\n",
    "n_songs= len(song_paths)\n",
    "print(len(song_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eef334a",
   "metadata": {},
   "outputs": [],
   "source": [
    "thr = int(0.9*n_songs)\n",
    "import random\n",
    "song_paths = random.sample(song_paths, n_songs)\n",
    "train_sp = song_paths[:thr]\n",
    "test_sp = song_paths[thr:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9995b2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(\"split_dataset\",\"train\")\n",
    "test_path = os.path.join(\"split_dataset\",\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1732c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "os.makedirs(test_path,exist_ok=True)\n",
    "os.makedirs(train_path,exist_ok=True)\n",
    "for idx, song in enumerate(train_sp):\n",
    "    shutil.copytree(os.path.join(\"song_chunks\",\"song_grouped\",song),os.path.join(train_path,song))\n",
    "    print(idx)\n",
    "for idx, song in enumerate(test_sp):\n",
    "    print(idx)\n",
    "    shutil.copytree(os.path.join(\"song_chunks\",\"song_grouped\",song),os.path.join(test_path,song))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a9d1fc",
   "metadata": {},
   "source": [
    "3. Apply window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22d0b5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "train_path = os.path.join(\"split_dataset\",\"train\")\n",
    "test_path = os.path.join(\"split_dataset\",\"test\")\n",
    "def get_file_paths(route):\n",
    "    name_list = []\n",
    "    for root, dirs, files in os.walk(route):\n",
    "        for file in files:\n",
    "            name_list.append(os.path.join(root, file))\n",
    "    if not name_list:\n",
    "        raise FileNotFoundError(\"\")\n",
    "    return name_list\n",
    "\n",
    "\n",
    "def merge_data(path):\n",
    "    file_paths = get_file_paths(path)\n",
    "    return [np.load(file_path) for file_path in file_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c95653f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_song_window(window_size,song):\n",
    "    sub_windows = (\n",
    "        # expand_dims are used to convert a 1D array to 2D array.\n",
    "        np.expand_dims(np.arange(window_size), 0) +\n",
    "        np.expand_dims(np.arange(song.shape[0]-window_size +1), 0).T\n",
    "    )\n",
    "    return np.array(song[sub_windows],dtype=np.int8).reshape(-1,window_size*4)\n",
    "\n",
    "def apply_window(window_size, data):\n",
    "    songs = [apply_song_window(window_size,song) for song in data]\n",
    "    return np.vstack(songs).reshape(-1,window_size,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9c405cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_and_shuffle(ws,dist_name):\n",
    "    data = train_data if dist_name==\"train\" else test_data\n",
    "    dataset = apply_window(ws,data)\n",
    "    np.random.shuffle(dataset)\n",
    "    dest_folder=os.path.join(\"windowed\",dist_name,str(ws))\n",
    "    os.makedirs(dest_folder,exist_ok=True)\n",
    "    dest = os.path.join(dest_folder,\"windows.npy\")\n",
    "    np.save(dest,dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a0c40e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62429\n"
     ]
    }
   ],
   "source": [
    "train_data = merge_data(train_path)\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa1e4416",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_and_shuffle(6, \"train\")\n",
    "window_and_shuffle(11, \"train\")\n",
    "window_and_save(21, \"train\")\n",
    "window_and_save(31, \"train\")\n",
    "window_and_save(41, \"train\")\n",
    "window_and_save(51, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3db0ae5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7165\n"
     ]
    }
   ],
   "source": [
    "test_data = merge_data(test_path)\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87f0e918",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_and_shuffle(6, \"test\")\n",
    "window_and_shuffle(11, \"test\")\n",
    "window_and_shuffle(21, \"test\")\n",
    "window_and_shuffle(31, \"test\")\n",
    "window_and_shuffle(41, \"test\")\n",
    "window_and_shuffle(51, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb78d83",
   "metadata": {},
   "source": [
    "4. Modest size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "295cb159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20877003, 6, 4)\n",
      "(20564858, 11, 4)\n",
      "(19940568, 21, 4)\n",
      "(19316278, 31, 4)\n",
      "(18691988, 41, 4)\n",
      "(18067698, 51, 4)\n",
      "(2366574, 6, 4)\n",
      "(2330749, 11, 4)\n",
      "(2259099, 21, 4)\n",
      "(2187449, 31, 4)\n",
      "(2115799, 41, 4)\n",
      "(2044149, 51, 4)\n"
     ]
    }
   ],
   "source": [
    "in_path = \"windowed\"\n",
    "import os\n",
    "import numpy as np\n",
    "def modest_size(ws,dist,n_ex):\n",
    "    n_ex=int(n_ex)\n",
    "    inp=os.path.join(in_path,dist,str(ws),\"windows.npy\")\n",
    "    outp=os.path.join(\"modest\",dist,str(ws))\n",
    "    os.makedirs(outp,exist_ok=True)\n",
    "    in_data=np.load(inp)\n",
    "    print(in_data.shape)\n",
    "    out_data=in_data[:n_ex,:,:]\n",
    "    np.save(os.path.join(outp,\"windows.npy\"),out_data)\n",
    "    \n",
    "    \n",
    "modest_size(6, \"train\",2e6)\n",
    "modest_size(11, \"train\",2e6)\n",
    "modest_size(21, \"train\",2e6)\n",
    "modest_size(31, \"train\",2e6)\n",
    "modest_size(41, \"train\",2e6)\n",
    "modest_size(51, \"train\",2e6)\n",
    "\n",
    "modest_size(6, \"test\",2e5)\n",
    "modest_size(11, \"test\",2e5)\n",
    "modest_size(21, \"test\",2e5)\n",
    "modest_size(31, \"test\",2e5)\n",
    "modest_size(41, \"test\",2e5)\n",
    "modest_size(51, \"test\",2e5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28055fe",
   "metadata": {},
   "source": [
    "5.Get weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3ebf0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000000, 6, 4)\n",
      "(2000000, 11, 4)\n",
      "(2000000, 21, 4)\n",
      "(2000000, 31, 4)\n",
      "(2000000, 41, 4)\n",
      "(2000000, 51, 4)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "in_path = os.path.join(\"modest\",\"train\")\n",
    "import numpy as np\n",
    "\n",
    "def single_feature_weights(data,ws,index):\n",
    "    data=data[:,:,index]\n",
    "    feature=[\"semitone\",\"octave\",\"dur_log\",\"dotted\"][index]\n",
    "    _,freqs=np.unique(data,return_counts=True)\n",
    "    i_freqs = i_freqs = np.divide(1., freqs, out=np.zeros_like(freqs, dtype='float'), where=freqs != 0)\n",
    "    weight_vector = freqs.shape[0] * i_freqs / np.sum(i_freqs)\n",
    "    out_path=os.path.join(in_path,str(ws),f'{feature}_weights.npy')\n",
    "    np.save(out_path,weight_vector)\n",
    "    \n",
    "def extract_all_weights(labels,ws):\n",
    "    single_feature_weights(labels,ws,0)\n",
    "    single_feature_weights(labels,ws,1)\n",
    "    single_feature_weights(labels,ws,2)\n",
    "    single_feature_weights(labels,ws,3)\n",
    "def get_and_save_weights(ex_size):\n",
    "    in_file = os.path.join(in_path,str(ex_size),\"windows.npy\")\n",
    "    data = np.load(in_file)\n",
    "    ws=ex_size-1\n",
    "    print(data.shape)\n",
    "    labels=data[:,ws::ex_size,:]\n",
    "    extract_all_weights(labels,ex_size)\n",
    "get_and_save_weights(6)\n",
    "get_and_save_weights(11)\n",
    "get_and_save_weights(21)\n",
    "get_and_save_weights(31)\n",
    "get_and_save_weights(41)\n",
    "get_and_save_weights(51)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c86309",
   "metadata": {},
   "source": [
    "6 check model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de3c7672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path: /dotted/dotted/bias:0\n",
      "Shape: (2,)\n",
      "Data type: float32\n",
      "-0.8876514 0.887647\n",
      "Path: /dotted/dotted/kernel:0\n",
      "Shape: (128, 2)\n",
      "Data type: float32\n",
      "-1.2200236 1.272265\n",
      "Path: /dur_log/dur_log/bias:0\n",
      "Shape: (7,)\n",
      "Data type: float32\n",
      "-2.9435296 1.1916081\n",
      "Path: /dur_log/dur_log/kernel:0\n",
      "Shape: (128, 7)\n",
      "Data type: float32\n",
      "-7.0105762 1.1719713\n",
      "Path: /embedding_4/embedding_4/embeddings:0\n",
      "Shape: (13, 8)\n",
      "Data type: float32\n",
      "-1.6156926 1.6451619\n",
      "Path: /embedding_5/embedding_5/embeddings:0\n",
      "Shape: (11, 8)\n",
      "Data type: float32\n",
      "-1.3608106 1.6911125\n",
      "Path: /embedding_6/embedding_6/embeddings:0\n",
      "Shape: (7, 8)\n",
      "Data type: float32\n",
      "-1.7391678 2.693332\n",
      "Path: /embedding_7/embedding_7/embeddings:0\n",
      "Shape: (2, 8)\n",
      "Data type: float32\n",
      "-0.70769805 1.7395235\n",
      "Path: /lstm/lstm/lstm_cell/bias:0\n",
      "Shape: (512,)\n",
      "Data type: float32\n",
      "-0.9392935 1.711859\n",
      "Path: /lstm/lstm/lstm_cell/kernel:0\n",
      "Shape: (32, 512)\n",
      "Data type: float32\n",
      "-3.134302 3.0694928\n",
      "Path: /lstm/lstm/lstm_cell/recurrent_kernel:0\n",
      "Shape: (128, 512)\n",
      "Data type: float32\n",
      "-2.740366 2.5098338\n",
      "Path: /octave/octave/bias:0\n",
      "Shape: (11,)\n",
      "Data type: float32\n",
      "-4.4708734 -0.5121108\n",
      "Path: /octave/octave/kernel:0\n",
      "Shape: (128, 11)\n",
      "Data type: float32\n",
      "-6.6418962 0.938066\n",
      "Path: /semitone/semitone/bias:0\n",
      "Shape: (13,)\n",
      "Data type: float32\n",
      "-1.0402938 0.70899814\n",
      "Path: /semitone/semitone/kernel:0\n",
      "Shape: (128, 13)\n",
      "Data type: float32\n",
      "-2.7596033 0.9133311\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "def traverse_datasets(hdf_file):\n",
    "\n",
    "    def h5py_dataset_iterator(g, prefix=''):\n",
    "        for key in g.keys():\n",
    "            item = g[key]\n",
    "            path = f'{prefix}/{key}'\n",
    "            if isinstance(item, h5py.Dataset): # test for dataset\n",
    "                yield (path, item)\n",
    "            elif isinstance(item, h5py.Group): # test for group (go down)\n",
    "                yield from h5py_dataset_iterator(item, path)\n",
    "\n",
    "    for path, _ in h5py_dataset_iterator(hdf_file):\n",
    "        yield path\n",
    "        \n",
    "import os\n",
    "weight_path=os.path.join(\"..\",\"report\",\"1(2022-04-19-21-31-53)\",\"weights\",\"lstm.h5\")\n",
    "\n",
    "f = h5py.File(weight_path, 'r')\n",
    "for dset in traverse_datasets(f):\n",
    "    print('Path:', dset)\n",
    "    print('Shape:', f[dset].shape)\n",
    "    print('Data type:', f[dset].dtype)\n",
    "    print(np.array(f[dset]).min(),np.array(f[dset]).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac5265b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.740366 2.5098338\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45368d1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae057a8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
